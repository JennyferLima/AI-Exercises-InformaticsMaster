{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # Problem 7\n# \n# #### Regarding Decision Tree Induction, present an application with a dataset containing continuous attributes that serves to illustrate and discuss the presence of overfitting (and/or underfitting) and how the algorithm handles it. Run and showcase the application using Scikit Learn.\n\n# %% [markdown]\n# ## Decision Tree Application with Iris Dataset\n\n# %% [markdown]\n# Step 1: Importing necessary libraries\n# \n# In this step, the necessary libraries were imported for the development of the application. They are:\n# \n# * NumPy and Pandas for data manipulation and analysis.\n# * `load_iris` from Scikit Learn to load the Iris dataset.\n# * `train_test_split` to split the data into training and testing sets.\n# * `DecisionTreeClassifier` to create and train the decision tree model.\n# * `accuracy_score` to calculate the model accuracy.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-04-03T18:46:10.111590Z\",\"iopub.execute_input\":\"2024-04-03T18:46:10.112070Z\",\"iopub.status.idle\":\"2024-04-03T18:46:10.118431Z\",\"shell.execute_reply.started\":\"2024-04-03T18:46:10.112037Z\",\"shell.execute_reply\":\"2024-04-03T18:46:10.117149Z\"}}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# %% [markdown]\n# Step 2: Loading the Iris dataset\n# \n# In this step, the `load_iris()` function from Scikit Learn is used to load the Iris dataset. The attributes of the dataset include the length and width of the sepal and petal of the flowers.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-04-03T18:46:10.120708Z\",\"iopub.execute_input\":\"2024-04-03T18:46:10.121319Z\",\"iopub.status.idle\":\"2024-04-03T18:46:10.134717Z\",\"shell.execute_reply.started\":\"2024-04-03T18:46:10.121279Z\",\"shell.execute_reply\":\"2024-04-03T18:46:10.133737Z\"}}\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# %% [markdown]\n# Step 3: Splitting the data into training and testing sets\n# \n# Here, the data is split into training and testing sets using the `train_test_split()` function from Scikit Learn. The test set will represent 20% of the data, while the training set will represent 80%.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-04-03T18:46:10.136009Z\",\"iopub.execute_input\":\"2024-04-03T18:46:10.136567Z\",\"iopub.status.idle\":\"2024-04-03T18:46:10.143788Z\",\"shell.execute_reply.started\":\"2024-04-03T18:46:10.136515Z\",\"shell.execute_reply\":\"2024-04-03T18:46:10.142875Z\"}}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# %% [markdown]\n# Step 4: Creating and training a decision tree model\n# \n# In this step, a decision tree model is created and trained using the `DecisionTreeClassifier()` class from Scikit Learn. The model is trained with the training data, and predictions are made for both the training and testing sets.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-04-03T18:46:10.145102Z\",\"iopub.execute_input\":\"2024-04-03T18:46:10.145505Z\",\"iopub.status.idle\":\"2024-04-03T18:46:10.161635Z\",\"shell.execute_reply.started\":\"2024-04-03T18:46:10.145476Z\",\"shell.execute_reply\":\"2024-04-03T18:46:10.160316Z\"}}\n# Initial model without hyperparameter tuning\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Making predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Calculating accuracy\ntrain_accuracy = accuracy_score(y_train, y_pred_train)\ntest_accuracy = accuracy_score(y_test, y_pred_test)\n\nprint(\"Accuracy on training set:\", train_accuracy)\nprint(\"Accuracy on test set:\", test_accuracy)\n\n# %% [markdown]\n# This output indicates that the model has an accuracy of 100% on both the training and test data. This suggests the presence of overfitting, where the model fits the training data too well but does not generalize well to new data.\n\n# %% [markdown]\n# Step 5: Evaluating the presence of overfitting\n# \n# The difference between the accuracies on the training and testing sets is observed to evaluate the presence of overfitting. A large disparity between the two accuracies suggests the presence of overfitting.\n\n# %% [markdown]\n# Step 6: Dealing with overfitting by adjusting hyperparameters\n# \n# To address overfitting, the model's hyperparameters are adjusted. In this case, the maximum depth of the decision tree was adjusted to 3.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-04-03T18:46:10.163638Z\",\"iopub.execute_input\":\"2024-04-03T18:46:10.164384Z\",\"iopub.status.idle\":\"2024-04-03T18:46:10.174036Z\",\"shell.execute_reply.started\":\"2024-04-03T18:46:10.164350Z\",\"shell.execute_reply\":\"2024-04-03T18:46:10.172600Z\"}}\n# Model with hyperparameter tuning (maximum depth)\nmodel_pruned = DecisionTreeClassifier(max_depth=3)\nmodel_pruned.fit(X_train, y_train)\n\n# Making predictions\ny_pred_train_pruned = model_pruned.predict(X_train)\ny_pred_test_pruned = model_pruned.predict(X_test)\n\n# Calculating accuracy\ntrain_accuracy_pruned = accuracy_score(y_train, y_pred_train_pruned)\ntest_accuracy_pruned = accuracy_score(y_test, y_pred_test_pruned)\n\nprint(\"Accuracy on training set (with tuning):\", train_accuracy_pruned)\nprint(\"Accuracy on test set (with tuning):\", test_accuracy_pruned)\n\n# %% [markdown]\n# This output shows that after adjusting the hyperparameters, the accuracy on the training set decreased slightly, while the accuracy on the test set remained high. This suggests that the model is less prone to overfitting and is better able to generalize to new data.\n\n# %% [markdown]\n# ## Conclusion\n# \n# A practical application of decision trees was developed using the Iris dataset. The presence of overfitting was demonstrated, and methods for dealing with it by adjusting the model's hyperparameters were discussed. The adjustment of hyperparameters resulted in a more robust and generalizable model.","metadata":{"_uuid":"87392ada-ff8f-4276-99dd-323b2b3cf589","_cell_guid":"69c3a74c-a571-438b-b0ea-c755b546e432","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-03T18:47:46.167923Z","iopub.execute_input":"2024-04-03T18:47:46.168410Z","iopub.status.idle":"2024-04-03T18:47:46.189882Z","shell.execute_reply.started":"2024-04-03T18:47:46.168377Z","shell.execute_reply":"2024-04-03T18:47:46.188803Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy on training set: 1.0\nAccuracy on test set: 1.0\nAccuracy on training set (with tuning): 0.9583333333333334\nAccuracy on test set (with tuning): 1.0\n","output_type":"stream"}]}]}